{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Deep Convolutional Neural Network for NLP/ sentiment analysis</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>1. Importing/loading libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>2. Data preprocessing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 1992\n",
      "Size of test data: 498\n",
      "Training data records: \n",
      "   sentiment  id                          date    query          user  \\\n",
      "0          4   3  Mon May 11 03:17:40 UTC 2009  kindle2        tpryan   \n",
      "1          4   4  Mon May 11 03:18:03 UTC 2009  kindle2        vcu451   \n",
      "2          4   5  Mon May 11 03:18:54 UTC 2009  kindle2        chadfu   \n",
      "3          4   6  Mon May 11 03:19:04 UTC 2009  kindle2         SIX15   \n",
      "4          4   7  Mon May 11 03:21:41 UTC 2009  kindle2      yamarama   \n",
      "5          4   8  Mon May 11 03:22:00 UTC 2009  kindle2  GeorgeVHulme   \n",
      "6          0   9  Mon May 11 03:22:30 UTC 2009      aig       Seth937   \n",
      "7          4  10  Mon May 11 03:26:10 UTC 2009   jquery     dcostalis   \n",
      "8          4  11  Mon May 11 03:27:15 UTC 2009  twitter       PJ_King   \n",
      "9          4  12  Mon May 11 03:29:20 UTC 2009    obama   mandanicole   \n",
      "\n",
      "                                                text  \n",
      "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
      "1  Reading my kindle2...  Love it... Lee childs i...  \n",
      "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
      "3  @kenburbary You'll love your Kindle2. I've had...  \n",
      "4  @mikefish  Fair enough. But i have the Kindle2...  \n",
      "5  @richardebaker no. it is too big. I'm quite ha...  \n",
      "6  Fuck this economy. I hate aig and their non lo...  \n",
      "7                      Jquery is my new best friend.  \n",
      "8                                      Loves twitter  \n",
      "9  how can you not love Obama? he makes jokes abo...  \n",
      "\n",
      "Test data 10 records: \n",
      "   sentiment  id                          date    query          user  \\\n",
      "0          4   3  Mon May 11 03:17:40 UTC 2009  kindle2        tpryan   \n",
      "1          4   4  Mon May 11 03:18:03 UTC 2009  kindle2        vcu451   \n",
      "2          4   5  Mon May 11 03:18:54 UTC 2009  kindle2        chadfu   \n",
      "3          4   6  Mon May 11 03:19:04 UTC 2009  kindle2         SIX15   \n",
      "4          4   7  Mon May 11 03:21:41 UTC 2009  kindle2      yamarama   \n",
      "5          4   8  Mon May 11 03:22:00 UTC 2009  kindle2  GeorgeVHulme   \n",
      "6          0   9  Mon May 11 03:22:30 UTC 2009      aig       Seth937   \n",
      "7          4  10  Mon May 11 03:26:10 UTC 2009   jquery     dcostalis   \n",
      "8          4  11  Mon May 11 03:27:15 UTC 2009  twitter       PJ_King   \n",
      "9          4  12  Mon May 11 03:29:20 UTC 2009    obama   mandanicole   \n",
      "\n",
      "                                                text  \n",
      "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
      "1  Reading my kindle2...  Love it... Lee childs i...  \n",
      "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
      "3  @kenburbary You'll love your Kindle2. I've had...  \n",
      "4  @mikefish  Fair enough. But i have the Kindle2...  \n",
      "5  @richardebaker no. it is too big. I'm quite ha...  \n",
      "6  Fuck this economy. I hate aig and their non lo...  \n",
      "7                      Jquery is my new best friend.  \n",
      "8                                      Loves twitter  \n",
      "9  how can you not love Obama? he makes jokes abo...  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "\n",
    "training_data = pd.read_csv(\"training_data.csv\", header = None, names = columns, engine = \"python\", encoding = \"latin1\")\n",
    "test_data = pd.read_csv(\"test_data.csv\", header = None, names = columns, engine = \"python\", encoding = \"latin1\")\n",
    "\n",
    "size_of_training_data = len(training_data)\n",
    "size_of_test_data = len(test_data)\n",
    "\n",
    "print(f\"Size of training data: {size_of_training_data}\")\n",
    "print(f\"Size of test data: {size_of_test_data}\")\n",
    "print(f\"Training data records: \\n{training_data.head(10)}\\n\")\n",
    "print(f\"Test data 10 records: \\n{test_data.head(10)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.drop([\"id\", \"date\", \"query\", \"user\"], axis = 1, inplace = True)\n",
    "test_data.drop([\"id\", \"date\", \"query\", \"user\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "      sentiment                                               text\n",
      "0             4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
      "1             4  Reading my kindle2...  Love it... Lee childs i...\n",
      "2             4  Ok, first assesment of the #kindle2 ...it fuck...\n",
      "3             4  @kenburbary You'll love your Kindle2. I've had...\n",
      "4             4  @mikefish  Fair enough. But i have the Kindle2...\n",
      "...         ...                                                ...\n",
      "1987          2  Ask Programming: LaTeX or InDesign?: submitted...\n",
      "1988          0  On that note, I hate Word. I hate Pages. I hat...\n",
      "1989          4  Ahhh... back in a *real* text editing environm...\n",
      "1990          0  Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
      "1991          0  Reading the tweets coming out of Iran... The w...\n",
      "\n",
      "[1992 rows x 2 columns]\n",
      "\n",
      "Test data: \n",
      "     sentiment                                               text\n",
      "0            4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
      "1            4  Reading my kindle2...  Love it... Lee childs i...\n",
      "2            4  Ok, first assesment of the #kindle2 ...it fuck...\n",
      "3            4  @kenburbary You'll love your Kindle2. I've had...\n",
      "4            4  @mikefish  Fair enough. But i have the Kindle2...\n",
      "..         ...                                                ...\n",
      "493          2  Ask Programming: LaTeX or InDesign?: submitted...\n",
      "494          0  On that note, I hate Word. I hate Pages. I hat...\n",
      "495          4  Ahhh... back in a *real* text editing environm...\n",
      "496          0  Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
      "497          0  Reading the tweets coming out of Iran... The w...\n",
      "\n",
      "[498 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data: \\n{training_data}\\n\")\n",
    "print(f\"Test data: \\n{test_data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PanCh\\AppData\\Local\\Temp\\ipykernel_1420\\597905739.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  tweet = BeautifulSoup(tweet, 'lxml').get_text()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\PanCh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m training_data[column_name] \u001b[38;5;241m=\u001b[39m training_data[column_name]\u001b[38;5;241m.\u001b[39mapply(clean_tweet)\n\u001b[0;32m     11\u001b[0m test_data[column_name] \u001b[38;5;241m=\u001b[39m test_data[column_name]\u001b[38;5;241m.\u001b[39mapply(clean_tweet)\n\u001b[1;32m---> 12\u001b[0m test_data[\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining_data: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtraining_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest_data: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtest_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PanCh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\PanCh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-0./]+\", ' ', tweet)\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    tweet = re.sub(r\" +\", \" \", tweet)\n",
    "    return tweet\n",
    "\n",
    "column_name = \"text\"\n",
    "training_data[column_name] = training_data[column_name].apply(clean_tweet)\n",
    "test_data[column_name] = test_data[column_name].apply(clean_tweet)\n",
    "test_data[test_data[\"label\"] == 4] = 1\n",
    "\n",
    "print(f\"Training_data: \\n{training_data}\\n\")\n",
    "print(f\"Test_data: \\n{test_data}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
